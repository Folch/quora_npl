{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "wcIv5V4GgZyK",
    "outputId": "502f38a9-bfd7-4ffe-8523-2d9bcedd0c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.15.4)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: boto3 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (1.9.50)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (2.19.1)\n",
      "Requirement already satisfied: bz2file in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.50 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.50)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (2.7)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (1.23)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.50->boto3->smart-open>=1.2.1->gensim) (2.7.3)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.50->boto3->smart-open>=1.2.1->gensim) (0.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spacy 2.0.12 has requirement cymem<1.32,>=1.30, but you'll have cymem 2.0.2 which is incompatible.\n",
      "spacy 2.0.12 has requirement murmurhash<0.29,>=0.28, but you'll have murmurhash 1.0.1 which is incompatible.\n",
      "spacy 2.0.12 has requirement preshed<2.0.0,>=1.0.0, but you'll have preshed 2.0.1 which is incompatible.\n",
      "spacy 2.0.12 has requirement regex==2017.4.5, but you'll have regex 2017.11.9 which is incompatible.\n",
      "spacy 2.0.12 has requirement thinc<6.11.0,>=6.10.3, but you'll have thinc 6.12.0 which is incompatible.\n",
      "thinc 6.12.0 has requirement msgpack-numpy<0.4.4.0, but you'll have msgpack-numpy 0.4.4.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in c:\\programdata\\anaconda3\\lib\\site-packages (0.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thinc 6.12.0 has requirement msgpack-numpy<0.4.4.0, but you'll have msgpack-numpy 0.4.4.1 which is incompatible.\n",
      "spacy 2.0.12 has requirement cymem<1.32,>=1.30, but you'll have cymem 2.0.2 which is incompatible.\n",
      "spacy 2.0.12 has requirement murmurhash<0.29,>=0.28, but you'll have murmurhash 1.0.1 which is incompatible.\n",
      "spacy 2.0.12 has requirement preshed<2.0.0,>=1.0.0, but you'll have preshed 2.0.1 which is incompatible.\n",
      "spacy 2.0.12 has requirement regex==2017.4.5, but you'll have regex 2017.11.9 which is incompatible.\n",
      "spacy 2.0.12 has requirement thinc<6.11.0,>=6.10.3, but you'll have thinc 6.12.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\programdata\\anaconda3\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.3)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thinc 6.12.0 has requirement msgpack-numpy<0.4.4.0, but you'll have msgpack-numpy 0.4.4.1 which is incompatible.\n",
      "spacy 2.0.12 has requirement cymem<1.32,>=1.30, but you'll have cymem 2.0.2 which is incompatible.\n",
      "spacy 2.0.12 has requirement murmurhash<0.29,>=0.28, but you'll have murmurhash 1.0.1 which is incompatible.\n",
      "spacy 2.0.12 has requirement preshed<2.0.0,>=1.0.0, but you'll have preshed 2.0.1 which is incompatible.\n",
      "spacy 2.0.12 has requirement regex==2017.4.5, but you'll have regex 2017.11.9 which is incompatible.\n",
      "spacy 2.0.12 has requirement thinc<6.11.0,>=6.10.3, but you'll have thinc 6.12.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim\n",
    "!pip3 install fuzzywuzzy\n",
    "!pip3 install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "mQkhb0E4gcZl",
    "outputId": "c519fb43-9d32-467e-a24f-34a604a839c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.similarities import WmdSimilarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.matutils import softcossim\n",
    "from gensim.models import Word2Vec\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from textblob import Word\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "!python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')  \n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "thc3DGkcgt3x",
    "outputId": "dff144ed-4547-4e69-dcf4-71a282117a27"
   },
   "outputs": [],
   "source": [
    "## Load the Drive helper and mount\n",
    "#from google.colab import drive\n",
    "#\n",
    "## This will prompt for authorization.\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIQWIJq_xmRo"
   },
   "source": [
    "# **Loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRHnUdBGhDSY"
   },
   "outputs": [],
   "source": [
    "train_path = os.path.join('..','data','test_data_v3_processed.csv')\n",
    "train_data = pd.read_csv(train_path)\n",
    "#train_data = train_data.dropna(how=\"any\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "colab_type": "code",
    "id": "nZfBxZcM8DoO",
    "outputId": "12ebf7f0-c5e6-4148-a1e8-6a611f9f0977"
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iq7gIwl9xsYL"
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ympwxsUbs4Wr"
   },
   "outputs": [],
   "source": [
    "QUESTION1 = 'question1_lemma'\n",
    "QUESTION2 = 'question2_lemma'\n",
    "\n",
    "QUESTION1_original = 'question1'\n",
    "QUESTION2_original = 'question2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqVDm-r3hN8L"
   },
   "outputs": [],
   "source": [
    "## TODO DELETE\n",
    "#train_data['question1_lemma'] = train_data['question1']\n",
    "#train_data['question2_lemma'] = train_data['question2']\n",
    "#train_data = train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ea1An26khvdF"
   },
   "source": [
    "# **Fuzzy features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IoCVY3vh2Nr"
   },
   "outputs": [],
   "source": [
    "def setFuzzyFeatures(df):  \n",
    "    df['fuzz_ratio'] = df.apply(lambda x: fuzz.ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)\n",
    "    df['fuzz_partial_ratio'] = df.apply(lambda x: fuzz.partial_ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)\n",
    "    df['fuzz_partial_token_set_ratio'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)\n",
    "    df['fuzz_partial_token_sort_ratio'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)\n",
    "    df['fuzz_token_set_ratio'] = df.apply(lambda x: fuzz.token_set_ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)\n",
    "    df['fuzz_token_sort_ratio'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFWtoHdFjXYH"
   },
   "source": [
    "# **String features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8eJPiDtejVWI"
   },
   "outputs": [],
   "source": [
    "def _avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "def _numStopWords(sentence):\n",
    "    return len([x for x in sentence.split() if x in stop_words])\n",
    "\n",
    "def _numNumbers(sentence):\n",
    "    return len([x for x in sentence.split() if x.isdigit()])\n",
    "\n",
    "def _numUppercaseWords(sentence):\n",
    "    return len([x for x in sentence.split() if x.isupper()])\n",
    "\n",
    "def setStringFeatures(df):  \n",
    "    df['len_q1'] = df.question1.apply(lambda x: len(str(x)))\n",
    "    df['len_q2'] = df.question2.apply(lambda x: len(str(x)))\n",
    "    df['diff_len'] = df.len_q1 - df.len_q2\n",
    "    df['len_char_q1'] = df.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    df['len_char_q2'] = df.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    df['len_word_q1'] = df.question1.apply(lambda x: len(str(x).split()))\n",
    "    df['len_word_q2'] = df.question2.apply(lambda x: len(str(x).split()))\n",
    "    df['common_words'] = df.apply(lambda x: len(set(str(x[QUESTION1]).lower().split()).intersection(set(str(x[QUESTION2]).lower().split()))), axis=1)\n",
    "    df['avg_word_q1'] = df.question1.apply(lambda x: _avg_word(x))\n",
    "    df['avg_word_q2'] = df.question2.apply(lambda x: _avg_word(x))\n",
    "    df['num_stop_words_q1'] = df.question1.apply(lambda x: _numStopWords(x))\n",
    "    df['num_stop_words_q2'] = df.question2.apply(lambda x: _numStopWords(x))\n",
    "    df['numerics_q1'] = df.question1.apply(lambda x: _numNumbers(x))\n",
    "    df['numerics_q2'] = df.question2.apply(lambda x: _numNumbers(x))\n",
    "    df['uppercase_q1'] = df.question1.apply(lambda x: _numUppercaseWords(x))\n",
    "    df['uppercase_q2'] = df.question2.apply(lambda x: _numUppercaseWords(x))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IrqdfZPMpk0l"
   },
   "source": [
    "# **Word2Vec features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cz9B7_7purJ"
   },
   "outputs": [],
   "source": [
    "w2v_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JqcTURRpsw8"
   },
   "outputs": [],
   "source": [
    "def _getDictionary(df):  \n",
    "    documents = list(df[QUESTION1_original].apply(lambda x: x.split()))+list(df[QUESTION2_original].apply(lambda x: x.split()))\n",
    "    \n",
    "    return corpora.Dictionary(documents)\n",
    "   \n",
    "def _softCossim(row, dictionary, similarity_matrix):\n",
    "    q1 = row[QUESTION1_original].split()\n",
    "    q2 = row[QUESTION2_original].split()\n",
    "    \n",
    "    q1 = dictionary.doc2bow(q1)\n",
    "    q2 = dictionary.doc2bow(q2)\n",
    "    \n",
    "    return softcossim(q1, q2, similarity_matrix)\n",
    " \n",
    "def setWord2VecFeatures(df):\n",
    "    dictionary = _getDictionary(df)\n",
    "    similarity_matrix = w2v_model.similarity_matrix(dictionary)\n",
    "    \n",
    "    df['softcossim'] = df.apply(lambda row: _softCossim(row, dictionary, similarity_matrix), axis=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhC06hXRw0BQ"
   },
   "source": [
    "# **TextBlob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_rWf2qKw1vD"
   },
   "outputs": [],
   "source": [
    "def _numSpellingMistakes(sentence):\n",
    "    mistakes = 0\n",
    "    for word in sentence.split():\n",
    "    if TextBlob(word).correct() != word:\n",
    "        mistakes += 1\n",
    "    return mistakes\n",
    "\n",
    "\n",
    "def setTextBlobFeatures(df):\n",
    "    df['mistakes_q1'] = df.question1.apply(lambda x: _numSpellingMistakes(x))\n",
    "    df['mistakes_q2'] = df.question2.apply(lambda x: _numSpellingMistakes(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbUwzjaQ3HmE"
   },
   "source": [
    "# **Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6QbLzl3T3JHX"
   },
   "outputs": [],
   "source": [
    "def _getSpacySimilarity(row):\n",
    "    q1 = row[QUESTION1_original]\n",
    "    q2 = row[QUESTION2_original]\n",
    "    \n",
    "    tokens1 = nlp(Q1)\n",
    "    tokens2 = nlp(Q2)\n",
    "    \n",
    "    return tokens1.similarity(tokens2)\n",
    "\n",
    "def setSpacyFeatures(df):\n",
    "    df['spacy_sim'] = df.apply(lambda row: _getSpacySimilarity(row), axis=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6njwe-nw8Ys"
   },
   "source": [
    "# Merge Features **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZo6EjeZw_U1"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def saveDf(df):\n",
    "    df.set_index('test_id', inplace=True)\n",
    "    df.to_csv(path_or_buf=train_path, sep=',')\n",
    "\n",
    "\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60.\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "def setFeatures(df):\n",
    "    print('start setting features')\n",
    "    \n",
    "    #start_time = datetime.datetime.now()\n",
    "    #setFuzzyFeatures(df)\n",
    "    #end_time = datetime.datetime.now()\n",
    "    #seconds_elapsed = (end_time - start_time).total_seconds()\n",
    "    #print('finished setFuzzyFeatures')\n",
    "    #print(\"It took {} to execute this\".format(hms_string(seconds_elapsed)))\n",
    "    #\n",
    "    #start_time = datetime.datetime.now()\n",
    "    #setStringFeatures(df)\n",
    "    #end_time = datetime.datetime.now()\n",
    "    #seconds_elapsed = (end_time - start_time).total_seconds()\n",
    "    #print('finished setStringFeatures')\n",
    "    #print(\"It took {} to execute this\".format(hms_string(seconds_elapsed)))\n",
    "    #\n",
    "    start_time = datetime.datetime.now()\n",
    "    setWord2VecFeatures(df)\n",
    "    end_time = datetime.datetime.now()\n",
    "    seconds_elapsed = (end_time - start_time).total_seconds()\n",
    "    print('finished setWord2VecFeatures')\n",
    "    print(\"It took {} to execute this\".format(hms_string(seconds_elapsed)))\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    setTextBlobFeatures(df)\n",
    "    end_time = datetime.datetime.now()\n",
    "    seconds_elapsed = (end_time - start_time).total_seconds()\n",
    "    print('finished setTextBlobFeatures')\n",
    "    print(\"It took {} to execute this\".format(hms_string(seconds_elapsed)))\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    setSpacyFeatures(df)\n",
    "    end_time = datetime.datetime.now()\n",
    "    seconds_elapsed = (end_time - start_time).total_seconds()\n",
    "    print('finished setSpacyFeatures')\n",
    "    print(\"It took {} to execute this\".format(hms_string(seconds_elapsed)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKbdzMkfm4IC"
   },
   "source": [
    "# **Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgV9BW05m7_H"
   },
   "outputs": [],
   "source": [
    "df = train_data#.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1610
    },
    "colab_type": "code",
    "id": "itHCMS4YnAHG",
    "outputId": "01784865-f2e3-4d9c-861b-257ce33c95c8"
   },
   "outputs": [],
   "source": [
    "setFeatures(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LkAytFEInF7N"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDf(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4_feature_engineering.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
