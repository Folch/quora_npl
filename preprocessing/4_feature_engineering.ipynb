{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "wcIv5V4GgZyK",
    "outputId": "502f38a9-bfd7-4ffe-8523-2d9bcedd0c48"
   },
   "outputs": [],
   "source": [
    "!pip3 install gensim\n",
    "!pip3 install fuzzywuzzy\n",
    "!pip3 install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "mQkhb0E4gcZl",
    "outputId": "c519fb43-9d32-467e-a24f-34a604a839c4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.similarities import WmdSimilarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.matutils import softcossim\n",
    "from gensim.models import Word2Vec\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from textblob import Word\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "thc3DGkcgt3x",
    "outputId": "dff144ed-4547-4e69-dcf4-71a282117a27"
   },
   "outputs": [],
   "source": [
    "## Load the Drive helper and mount\n",
    "#from google.colab import drive\n",
    "#\n",
    "## This will prompt for authorization.\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIQWIJq_xmRo"
   },
   "source": [
    "# **Loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRHnUdBGhDSY"
   },
   "outputs": [],
   "source": [
    "train_path = os.path.join('..','data','train_data_v3_processed.csv')\n",
    "df_train = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join('..','data','test_data_v3_processed.csv')\n",
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train.columns))\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "colab_type": "code",
    "id": "nZfBxZcM8DoO",
    "outputId": "12ebf7f0-c5e6-4148-a1e8-6a611f9f0977"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iq7gIwl9xsYL"
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ympwxsUbs4Wr"
   },
   "outputs": [],
   "source": [
    "QUESTION1 = 'question1_lemma'\n",
    "QUESTION2 = 'question2_lemma'\n",
    "\n",
    "QUESTION1_original = 'question1'\n",
    "QUESTION2_original = 'question2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ea1An26khvdF"
   },
   "source": [
    "# **Fuzzy features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IoCVY3vh2Nr"
   },
   "outputs": [],
   "source": [
    "def setFuzzyFeatures(df):  \n",
    "    df['fuzz_ratio'] = df.apply(lambda x: fuzz.ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)\n",
    "    df['fuzz_partial_ratio'] = df.apply(lambda x: fuzz.partial_ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)\n",
    "    df['fuzz_partial_token_set_ratio'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)\n",
    "    df['fuzz_partial_token_sort_ratio'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)\n",
    "    df['fuzz_token_set_ratio'] = df.apply(lambda x: fuzz.token_set_ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)\n",
    "    df['fuzz_token_sort_ratio'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x[QUESTION1]), str(x[QUESTION2])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFWtoHdFjXYH"
   },
   "source": [
    "# **String features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for string features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "    \n",
    "ques = pd.concat([df_train[['question1', 'question2']], \\\n",
    "                  df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "    q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "    q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\n",
    "\n",
    "df_train_question1 = df_train['question1'].map(lambda x: str(x).lower().split())\n",
    "df_train_question2 = df_train['question2'].map(lambda x: str(x).lower().split())\n",
    "\n",
    "train_qs = pd.Series(df_train_question1.tolist() + df_train_question2.tolist())\n",
    "\n",
    "words = [x for y in train_qs for x in y]\n",
    "counts = Counter(words)\n",
    "weights = {word: _get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8eJPiDtejVWI"
   },
   "outputs": [],
   "source": [
    "def _word_match_share(row, stops=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2)) / (len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "\n",
    "def _jaccard(row):\n",
    "    wic = set(row['question1']).intersection(set(row['question2']))\n",
    "    uw = set(row['question1']).union(row['question2'])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "\n",
    "def _common_words(row):\n",
    "    return len(set(row['question1']).intersection(set(row['question2'])))\n",
    "\n",
    "\n",
    "def _total_unique_words(row):\n",
    "    return len(set(row['question1']).union(row['question2']))\n",
    "\n",
    "\n",
    "def _total_unq_words_stop(row, stops):\n",
    "    return len([x for x in set(row['question1']).union(row['question2']) if x not in stops])\n",
    "\n",
    "\n",
    "def _wc_diff(row):\n",
    "    return abs(len(row['question1']) - len(row['question2']))\n",
    "\n",
    "\n",
    "def _wc_ratio(row):\n",
    "    l1 = len(row['question1']) * 1.0\n",
    "    l2 = len(row['question2'])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "\n",
    "def _wc_diff_unique(row):\n",
    "    return abs(len(set(row['question1'])) - len(set(row['question2'])))\n",
    "\n",
    "\n",
    "def _wc_ratio_unique(row):\n",
    "    l1 = len(set(row['question1'])) * 1.0\n",
    "    l2 = len(set(row['question2']))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "\n",
    "def _wc_diff_unique_stop(row, stops=None):\n",
    "    return abs(len([x for x in set(row['question1']) if x not in stops]) - len(\n",
    "        [x for x in set(row['question2']) if x not in stops]))\n",
    "\n",
    "\n",
    "def _wc_ratio_unique_stop(row, stops=None):\n",
    "    l1 = len([x for x in set(row['question1']) if x not in stops]) * 1.0\n",
    "    l2 = len([x for x in set(row['question2']) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "\n",
    "def _same_start_word(row):\n",
    "    if not row['question1'] or not row['question2']:\n",
    "        return np.nan\n",
    "    return int(row['question1'][0] == row['question2'][0])\n",
    "\n",
    "\n",
    "def _char_diff(row):\n",
    "    return abs(len(''.join(row['question1'])) - len(''.join(row['question2'])))\n",
    "\n",
    "\n",
    "def _char_ratio(row):\n",
    "    l1 = len(''.join(row['question1']))\n",
    "    l2 = len(''.join(row['question2']))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "\n",
    "def _char_diff_unique_stop(row, stops=None):\n",
    "    return abs(len(''.join([x for x in set(row['question1']) if x not in stops])) - len(\n",
    "        ''.join([x for x in set(row['question2']) if x not in stops])))\n",
    "\n",
    "def _tfidf_word_match_share_stops(row, stops=None, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "\n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in\n",
    "                                                                                    q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "\n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "\n",
    "def _tfidf_word_match_share(row, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "\n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in\n",
    "                                                                                    q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "\n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "def _q1_freq(row):\n",
    "    return (len(q_dict[row['question1']]))\n",
    "\n",
    "def _q2_freq(row):\n",
    "    return (len(q_dict[row['question2']]))\n",
    "\n",
    "def _q1_q2_intersect(row):\n",
    "    return (len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "def _avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "def _numStopWords(sentence):\n",
    "    return len([x for x in sentence.split() if x in stop_words])\n",
    "\n",
    "def _numNumbers(sentence):\n",
    "    return len([x for x in sentence.split() if x.isdigit()])\n",
    "\n",
    "def _numUppercaseWords(sentence):\n",
    "    return len([x for x in sentence.split() if x.isupper()])\n",
    "\n",
    "def setStringFeatures(df):  \n",
    "    df['len_q1'] = df.question1.apply(lambda x: len(str(x)))\n",
    "    df['len_q2'] = df.question2.apply(lambda x: len(str(x)))\n",
    "    df['diff_len'] = df.len_q1 - df.len_q2\n",
    "    df['len_char_q1'] = df.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    df['len_char_q2'] = df.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    df['len_word_q1'] = df.question1.apply(lambda x: len(str(x).split()))\n",
    "    df['len_word_q2'] = df.question2.apply(lambda x: len(str(x).split()))\n",
    "    df['common_words'] = df.apply(lambda x: len(set(str(x[QUESTION1]).lower().split()).intersection(set(str(x[QUESTION2]).lower().split()))), axis=1)\n",
    "    df['avg_word_q1'] = df.question1.apply(lambda x: _avg_word(x))\n",
    "    df['avg_word_q2'] = df.question2.apply(lambda x: _avg_word(x))\n",
    "    df['num_stop_words_q1'] = df.question1.apply(lambda x: _numStopWords(x))\n",
    "    df['num_stop_words_q2'] = df.question2.apply(lambda x: _numStopWords(x))\n",
    "    df['numerics_q1'] = df.question1.apply(lambda x: _numNumbers(x))\n",
    "    df['numerics_q2'] = df.question2.apply(lambda x: _numNumbers(x))\n",
    "    df['uppercase_q1'] = df.question1.apply(lambda x: _numUppercaseWords(x))\n",
    "    df['uppercase_q2'] = df.question2.apply(lambda x: _numUppercaseWords(x))\n",
    "    df['char_ratio'] = df.apply(_char_ratio, axis=1, raw=True)  \n",
    "    df['jaccard'] = df.apply(_jaccard, axis=1, raw=True)  \n",
    "    df['wc_diff'] = df.apply(_wc_diff, axis=1, raw=True)  \n",
    "    df['wc_ratio'] = df.apply(_wc_ratio, axis=1, raw=True)  \n",
    "    df['wc_diff_unique'] = df.apply(_wc_diff_unique, axis=1, raw=True)  \n",
    "    df['wc_ratio_unique'] = df.apply(_wc_ratio_unique, axis=1, raw=True) \n",
    "    df['same_start'] = df.apply(_same_start_word, axis=1, raw=True)  \n",
    "    df['char_diff'] = df.apply(_char_diff, axis=1, raw=True)  \n",
    "    df['common_words'] = df.apply(_common_words, axis=1, raw=True)  \n",
    "    df['total_unique_words'] = df.apply(_total_unique_words, axis=1, raw=True)  \n",
    "    df['q1_q2_intersect'] = df.apply(_q1_q2_intersect, axis=1, raw=True)\n",
    "    df['q1_freq'] = df.apply(_q1_freq, axis=1, raw=True)\n",
    "    df['q2_freq'] = df.apply(_q2_freq, axis=1, raw=True)\n",
    "\n",
    "    f = functools.partial(_word_match_share, stops=stop_words)\n",
    "    df['word_match'] = df.apply(f, axis=1, raw=True)\n",
    "    \n",
    "    f = functools.partial(_tfidf_word_match_share, weights=weights)\n",
    "    df['tfidf_wm'] = df.apply(f, axis=1, raw=True) \n",
    "    \n",
    "    f = functools.partial(_tfidf_word_match_share_stops, stops=stop_words, weights=weights)\n",
    "    df['tfidf_wm_stops'] = df.apply(f, axis=1, raw=True)  \n",
    "    \n",
    "    f = functools.partial(_wc_diff_unique_stop, stops=stop_words)\n",
    "    df['wc_diff_unq_stop'] = df.apply(f, axis=1, raw=True)  \n",
    "    \n",
    "    f = functools.partial(_wc_ratio_unique_stop, stops=stop_words)\n",
    "    df['wc_ratio_unique_stop'] = df.apply(f, axis=1, raw=True)  \n",
    "    \n",
    "    f = functools.partial(_char_diff_unique_stop, stops=stop_words)\n",
    "    df['char_diff_unq_stop'] = df.apply(f, axis=1, raw=True)  \n",
    "    \n",
    "    f = functools.partial(_total_unq_words_stop, stops=stop_words)\n",
    "    df['total_unq_words_stop'] = df.apply(f, axis=1, raw=True)  \n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IrqdfZPMpk0l"
   },
   "source": [
    "# **Word2Vec features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cz9B7_7purJ"
   },
   "outputs": [],
   "source": [
    "path_google_news = os.path.join('..','data','GoogleNews-vectors-negative300.bin.gz')\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(path_google_news, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JqcTURRpsw8"
   },
   "outputs": [],
   "source": [
    "def _getDictionary(df):  \n",
    "    documents = list(df[QUESTION1_original].apply(lambda x: x.split()))+list(df[QUESTION2_original].apply(lambda x: x.split()))\n",
    "    \n",
    "    return corpora.Dictionary(documents)\n",
    "   \n",
    "def _softCossim(row, dictionary, similarity_matrix):\n",
    "    q1 = row[QUESTION1_original].split()\n",
    "    q2 = row[QUESTION2_original].split()\n",
    "    \n",
    "    q1 = dictionary.doc2bow(q1)\n",
    "    q2 = dictionary.doc2bow(q2)\n",
    "    \n",
    "    return softcossim(q1, q2, similarity_matrix)\n",
    " \n",
    "def setWord2VecFeatures(df):\n",
    "    dictionary = _getDictionary(df)\n",
    "    similarity_matrix = w2v_model.similarity_matrix(dictionary)\n",
    "    \n",
    "    df['softcossim'] = df.apply(lambda row: _softCossim(row, dictionary, similarity_matrix), axis=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhC06hXRw0BQ"
   },
   "source": [
    "# **TextBlob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_rWf2qKw1vD"
   },
   "outputs": [],
   "source": [
    "def _numSpellingMistakes(sentence):\n",
    "    mistakes = 0\n",
    "    for word in sentence.split():\n",
    "        if TextBlob(word).correct() != word:\n",
    "            mistakes += 1\n",
    "    return mistakes\n",
    "\n",
    "\n",
    "def setTextBlobFeatures(df):\n",
    "    df['mistakes_q1'] = df.question1.apply(lambda x: _numSpellingMistakes(x))\n",
    "    df['mistakes_q2'] = df.question2.apply(lambda x: _numSpellingMistakes(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbUwzjaQ3HmE"
   },
   "source": [
    "# **Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6QbLzl3T3JHX"
   },
   "outputs": [],
   "source": [
    "def _getSpacySimilarity(row):\n",
    "    q1 = row[QUESTION1_original]\n",
    "    q2 = row[QUESTION2_original]\n",
    "    \n",
    "    tokens1 = nlp(q1)\n",
    "    tokens2 = nlp(q2)\n",
    "    \n",
    "    return tokens1.similarity(tokens2)\n",
    "\n",
    "def setSpacyFeatures(df):\n",
    "    df['spacy_sim'] = df.apply(lambda row: _getSpacySimilarity(row), axis=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6njwe-nw8Ys"
   },
   "source": [
    "# Merge Features **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZo6EjeZw_U1"
   },
   "outputs": [],
   "source": [
    "def saveDf(df):\n",
    "    df2 = df.set_index('id')\n",
    "    df2.to_csv(path_or_buf=train_path, sep=',')\n",
    "\n",
    "\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60.\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "def setFeatures(df):\n",
    "    print('start setting features')\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    setFuzzyFeatures(df)\n",
    "    end_time = datetime.datetime.now()\n",
    "    seconds_elapsed = (end_time - start_time).total_seconds()\n",
    "    print('finished setFuzzyFeatures')\n",
    "    print(\"It took {} to execute this\".format(hms_string(seconds_elapsed)))\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    setStringFeatures(df)\n",
    "    end_time = datetime.datetime.now()\n",
    "    seconds_elapsed = (end_time - start_time).total_seconds()\n",
    "    print('finished setStringFeatures')\n",
    "    print(\"It took {} to execute this\".format(hms_string(seconds_elapsed)))\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    setWord2VecFeatures(df)\n",
    "    end_time = datetime.datetime.now()\n",
    "    seconds_elapsed = (end_time - start_time).total_seconds()\n",
    "    print('finished setWord2VecFeatures')\n",
    "    print(\"It took {} to execute this\".format(hms_string(seconds_elapsed)))\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    setTextBlobFeatures(df)\n",
    "    end_time = datetime.datetime.now()\n",
    "    seconds_elapsed = (end_time - start_time).total_seconds()\n",
    "    print('finished setTextBlobFeatures')\n",
    "    print(\"It took {} to execute this\".format(hms_string(seconds_elapsed)))\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    setSpacyFeatures(df)\n",
    "    end_time = datetime.datetime.now()\n",
    "    seconds_elapsed = (end_time - start_time).total_seconds()\n",
    "    print('finished setSpacyFeatures')\n",
    "    print(\"It took {} to execute this\".format(hms_string(seconds_elapsed)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKbdzMkfm4IC"
   },
   "source": [
    "# **Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgV9BW05m7_H"
   },
   "outputs": [],
   "source": [
    "df = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1610
    },
    "colab_type": "code",
    "id": "itHCMS4YnAHG",
    "outputId": "01784865-f2e3-4d9c-861b-257ce33c95c8"
   },
   "outputs": [],
   "source": [
    "setFeatures(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LkAytFEInF7N"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDf(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setFeatures(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDf(df_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4_feature_engineering.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
