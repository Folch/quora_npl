{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embedding_NN_model_v8.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "aXioKJ6V7kFP",
        "colab_type": "code",
        "outputId": "875c605e-a814-4454-9a4e-70ba37fa51a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D, BatchNormalization\n",
        "from keras.layers import Concatenate, Subtract, Multiply\n",
        "from keras.layers import Input, Dropout, PReLU, SpatialDropout1D\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import keras.backend as K\n",
        "from keras import optimizers\n",
        "\n",
        "import os\n",
        "import io"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "l6HoV37h7m-C",
        "colab_type": "code",
        "outputId": "b41344ee-9dfb-4dc5-daff-ceb1110af05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Loading drive content\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t1-hbHHz7rHi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = os.path.join('drive', 'My Drive', 'quora', 'train_data_v3_processed.csv')\n",
        "train_data = pd.read_csv(path, dtype={'question1': 'str', 'question2': 'str'})\n",
        "\n",
        "path = os.path.join('drive', 'My Drive', 'quora', 'test_data_v3_processed.csv')\n",
        "test_data = pd.read_csv(path, dtype={'question1': 'str', 'question2': 'str'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UhzkqsbS73hL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a9071cce-753d-4592-c7fd-dd2b92b85e81"
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip glove.840B.300d.zip -d ."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-08 21:11:00--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2018-12-08 21:11:01--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  8.64MB/s    in 3m 30s  \n",
            "\n",
            "2018-12-08 21:14:32 (9.88 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n",
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: ./glove.840B.300d.txt   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uJxPiNuEEcSr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RAW_Q1 = 0\n",
        "RAW_Q2 = 1\n",
        "LEMMA_Q1 = 2\n",
        "LEMMA_Q2 = 3\n",
        "TAGS_Q1 = 4\n",
        "TAGS_Q2 = 5\n",
        "IS_DUPLICATE = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uUN1R6IgEw7Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_questions(q1_train, q2_train, q1_val, q2_val, q1_test, q2_test):\n",
        "    tokenizer = Tokenizer(lower=False)\n",
        "    # Training the tokenizer with the words from all questions from training\n",
        "    tokenizer.fit_on_texts(np.concatenate((q1_train, q2_train), axis=0))\n",
        "\n",
        "    # Convert each word to a integer according to the tokenizer\n",
        "    q1_train = tokenizer.texts_to_sequences(q1_train)\n",
        "    q2_train = tokenizer.texts_to_sequences(q2_train)\n",
        "    q1_val = tokenizer.texts_to_sequences(q1_val)\n",
        "    q2_val = tokenizer.texts_to_sequences(q2_val)\n",
        "    q1_test = tokenizer.texts_to_sequences(q1_test)\n",
        "    q2_test = tokenizer.texts_to_sequences(q2_test)\n",
        "\n",
        "    # Add a left pad to make all the question have the same length\n",
        "    q1_train = pad_sequences(q1_train, maxlen=MAX_LENGTH)\n",
        "    q2_train = pad_sequences(q2_train, maxlen=MAX_LENGTH)\n",
        "    q1_val = pad_sequences(q1_val, maxlen=MAX_LENGTH)\n",
        "    q2_val = pad_sequences(q2_val, maxlen=MAX_LENGTH)\n",
        "    q1_test = pad_sequences(q1_test, maxlen=MAX_LENGTH)\n",
        "    q2_test = pad_sequences(q2_test, maxlen=MAX_LENGTH)\n",
        "    \n",
        "    return q1_train, q2_train, q1_val, q2_val, q1_test, q2_test, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q1oAiHb5BJXu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 35\n",
        "'''\n",
        "input:\n",
        "    - train: raw text of training\n",
        "    - test: raw text of testing\n",
        "ouput:\n",
        "    - train: processed training\n",
        "    - test: processed testing\n",
        "    - vocab: Number of vocaboluary\n",
        "'''\n",
        "def prep_data(train, val, test):\n",
        "    \n",
        "    raw_q1_train, raw_q2_train,\\\n",
        "    raw_q1_val, raw_q2_val,\\\n",
        "    raw_q1_test, raw_q2_test,\\\n",
        "    raw_tokenizer = process_questions(train[RAW_Q1],\n",
        "                                      train[RAW_Q2],\n",
        "                                      val[RAW_Q1],\n",
        "                                      val[RAW_Q2],\n",
        "                                      test[RAW_Q1],\n",
        "                                      test[RAW_Q2])\n",
        "    \n",
        "    lemma_q1_train, lemma_q2_train,\\\n",
        "    lemma_q1_val, lemma_q2_val,\\\n",
        "    lemma_q1_test, lemma_q2_test,\\\n",
        "    lemma_tokenizer = process_questions(train[LEMMA_Q1],\n",
        "                                        train[LEMMA_Q2],\n",
        "                                        val[LEMMA_Q1],\n",
        "                                        val[LEMMA_Q2],\n",
        "                                        test[LEMMA_Q1],\n",
        "                                        test[LEMMA_Q2])\n",
        "    \n",
        "    tags_q1_train, tags_q2_train,\\\n",
        "    tags_q1_val, tags_q2_val,\\\n",
        "    tags_q1_test, tags_q2_test,\\\n",
        "    tags_tokenizer = process_questions(train[TAGS_Q1],\n",
        "                                        train[TAGS_Q2],\n",
        "                                        val[TAGS_Q1],\n",
        "                                        val[TAGS_Q2],\n",
        "                                        test[TAGS_Q1],\n",
        "                                        test[TAGS_Q2])\n",
        "\n",
        "    train = raw_q1_train, raw_q2_train, lemma_q1_train, lemma_q2_train, tags_q1_train, tags_q2_train, train[IS_DUPLICATE]\n",
        "    val = raw_q1_val, raw_q2_val, lemma_q1_val, lemma_q2_val, tags_q1_val, tags_q2_val, val[IS_DUPLICATE]\n",
        "    test = raw_q1_test, raw_q2_test, lemma_q1_test, lemma_q2_test, tags_q1_test, tags_q2_test\n",
        "    \n",
        "    return train, val, test, raw_tokenizer, len(lemma_tokenizer.word_index) + 1, len(tags_tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BaZlHfayCgRX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prep_embd(fname, tokenizer):\n",
        "    f = open(fname,'r')\n",
        "    d = 300\n",
        "    \n",
        "    vocab = len(tokenizer.word_index) + 1\n",
        "    embedding_matrix = np.zeros((vocab, d))\n",
        "    \n",
        "    for line in f:\n",
        "        tokens = line.split(' ')\n",
        "        word = tokens[0]\n",
        "        if word in tokenizer.word_index:\n",
        "            i = tokenizer.word_index[word]\n",
        "            vector = np.asarray(tokens[1:], dtype='float32')\n",
        "            embedding_matrix[i] = vector\n",
        "            \n",
        "    return vocab, d, embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "brhoD9i3P2iR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_features(df):\n",
        "    df = df.drop(['question1', 'question2', \n",
        "                  'question1_lemma', 'question2_lemma', \n",
        "                  'question1_tag', 'question2_tag', \n",
        "                  'is_duplicate'], axis=1)\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop('id', axis=1)\n",
        "    elif 'test_id' in df.columns:\n",
        "        df = df.drop('test_id', axis=1)\n",
        "    return df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C7oE9X_sCiPN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = train_data[['question1', 'question2', \n",
        "                   'question1_lemma', 'question2_lemma', \n",
        "                   'question1_tag', 'question2_tag', \n",
        "                   'is_duplicate']].values\n",
        "\n",
        "train, val = train_test_split(data, test_size=0.2, random_state=19)\n",
        "train = train.T\n",
        "val = val.T\n",
        "\n",
        "test = test_data[['question1', 'question2', \n",
        "                  'question1_lemma', 'question2_lemma', \n",
        "                  'question1_tag', 'question2_tag']].values\n",
        "test = test.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvCXA58kHSPQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train, val, test, raw_tokenizer, LEMMA_VOCAB, TAGS_VOCAB = prep_data(train, val, test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q93H7PCDM-1Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = os.path.join('glove.840B.300d.txt')\n",
        "RAW_VOCAB, GLOVE_EMBEDDING_DIM, GLOVE_EMBEDDING_MATRIX = prep_embd(path, raw_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f735o3YlM-6x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 200\n",
        "DROPOUT_RATE = 0.25\n",
        "FILTERS = 16\n",
        "KERNEL_SIZE = 5\n",
        "\n",
        "def get_model_non_trainable_embeddings(input_q):\n",
        "    embd = Embedding(input_dim = RAW_VOCAB,\n",
        "                     output_dim = GLOVE_EMBEDDING_DIM, \n",
        "                     weights = [GLOVE_EMBEDDING_MATRIX],\n",
        "                     trainable = False,\n",
        "                     input_length=MAX_LENGTH)(input_q)\n",
        "    lstm = LSTM(96, \n",
        "                recurrent_dropout = DROPOUT_RATE)(embd)\n",
        "    return lstm\n",
        "\n",
        "def get_model_embeddings(input_q, vocab):\n",
        "    embd = Embedding(input_dim = vocab,\n",
        "                     output_dim = EMBEDDING_DIM, \n",
        "                     input_length=MAX_LENGTH)(input_q)\n",
        "    lstm = LSTM(96, \n",
        "                recurrent_dropout = DROPOUT_RATE)(embd)\n",
        "    \n",
        "    return lstm\n",
        "  \n",
        "def get_model_features(features_input):\n",
        "    model = BatchNormalization()(features_input)\n",
        "    for i in range(2):\n",
        "        model = Dense(units = 200, activation='relu')(model)\n",
        "        model = Dropout(DROPOUT_RATE)(model)\n",
        "    return model\n",
        "\n",
        "def get_model():\n",
        "    # Define inputs\n",
        "    raw_input_q1 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    raw_input_q2 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    \n",
        "    lemma_input_q1 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    lemma_input_q2 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    \n",
        "    tags_input_q1 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    tags_input_q2 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    \n",
        "    #features_input = Input(shape=(train_features.shape[1],), dtype='float32')\n",
        "   \n",
        "    # Load models\n",
        "    model_raw_q1 = get_model_non_trainable_embeddings(raw_input_q1)\n",
        "    model_raw_q2 = get_model_non_trainable_embeddings(raw_input_q2)\n",
        "    \n",
        "    model_lemma_q1 = get_model_embeddings(lemma_input_q1, LEMMA_VOCAB)\n",
        "    model_lemma_q2 = get_model_embeddings(lemma_input_q2, LEMMA_VOCAB)\n",
        "    \n",
        "    model_tags_q1 = get_model_embeddings(tags_input_q1, TAGS_VOCAB)\n",
        "    model_tags_q2 = get_model_embeddings(tags_input_q2, TAGS_VOCAB) \n",
        "    \n",
        "    #model_features = get_model_features(features_input)\n",
        "    \n",
        "    # Merge models\n",
        "    raw_subtract = Subtract()([model_raw_q2, model_raw_q1])\n",
        "    lemma_subtract = Subtract()([model_lemma_q2, model_lemma_q1])\n",
        "    tags_subtract = Subtract()([model_tags_q2, model_tags_q1])\n",
        "    \n",
        "    # mult = Multiply()([raw_subtract, lemma_subtract, tags_subtract])\n",
        "    \n",
        "    concat = Concatenate()([raw_subtract, lemma_subtract, tags_subtract])\n",
        "    concat = BatchNormalization()(concat)\n",
        "    concat = Dropout(DROPOUT_RATE)(concat)\n",
        "    \n",
        "    for i in range(2):\n",
        "        concat = Dense(units = 150, activation='relu')(concat)\n",
        "        concat = Dropout(DROPOUT_RATE)(concat)\n",
        "\n",
        "    output = Dense(1, activation='sigmoid')(concat)\n",
        "    \n",
        "    model = Model(inputs=[raw_input_q1, raw_input_q2, \n",
        "                          lemma_input_q1, lemma_input_q2, \n",
        "                          tags_input_q1, tags_input_q2], \n",
        "                  outputs=output)\n",
        "    \n",
        "    model.compile(optimizer='nadam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['binary_accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AZIEWAycM-_o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = get_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8MVK-m9j9tE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "e55b3353-128b-4221-99e6-498cb1c8a93f"
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_binary_accuracy', patience=5)\n",
        "model_checkpoint = ModelCheckpoint('embedding_NN_model_v7.h5', save_best_only=True, save_weights_only=False, monitor='val_loss', mode='min')\n",
        "\n",
        "model.fit([train[RAW_Q1], train[RAW_Q2], train[LEMMA_Q1], train[LEMMA_Q2], train[TAGS_Q1], train[TAGS_Q2]], train[IS_DUPLICATE],\n",
        "          validation_data = ([val[RAW_Q1], val[RAW_Q2], val[LEMMA_Q1], val[LEMMA_Q2], val[TAGS_Q1], val[TAGS_Q2]],val[IS_DUPLICATE]),\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=15,\n",
        "          callbacks=[early_stopping, model_checkpoint])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 258311 samples, validate on 64578 samples\n",
            "Epoch 1/15\n",
            "258311/258311 [==============================] - 206s 797us/step - loss: 0.5140 - binary_accuracy: 0.7449 - val_loss: 0.4740 - val_binary_accuracy: 0.7707\n",
            "Epoch 2/15\n",
            "258311/258311 [==============================] - 198s 765us/step - loss: 0.4219 - binary_accuracy: 0.7993 - val_loss: 0.5015 - val_binary_accuracy: 0.7619\n",
            "Epoch 3/15\n",
            "258311/258311 [==============================] - 199s 771us/step - loss: 0.3542 - binary_accuracy: 0.8354 - val_loss: 0.4600 - val_binary_accuracy: 0.7913\n",
            "Epoch 4/15\n",
            "258311/258311 [==============================] - 199s 771us/step - loss: 0.2977 - binary_accuracy: 0.8646 - val_loss: 0.5234 - val_binary_accuracy: 0.7892\n",
            "Epoch 5/15\n",
            "258311/258311 [==============================] - 199s 770us/step - loss: 0.2541 - binary_accuracy: 0.8862 - val_loss: 0.5355 - val_binary_accuracy: 0.7927\n",
            "Epoch 6/15\n",
            "258311/258311 [==============================] - 197s 763us/step - loss: 0.2211 - binary_accuracy: 0.9026 - val_loss: 0.5721 - val_binary_accuracy: 0.7925\n",
            "Epoch 7/15\n",
            "258311/258311 [==============================] - 198s 768us/step - loss: 0.1941 - binary_accuracy: 0.9158 - val_loss: 0.6510 - val_binary_accuracy: 0.7919\n",
            "Epoch 8/15\n",
            "258311/258311 [==============================] - 197s 761us/step - loss: 0.1722 - binary_accuracy: 0.9253 - val_loss: 0.7056 - val_binary_accuracy: 0.7985\n",
            "Epoch 9/15\n",
            "258311/258311 [==============================] - 198s 765us/step - loss: 0.1556 - binary_accuracy: 0.9338 - val_loss: 0.7492 - val_binary_accuracy: 0.7960\n",
            "Epoch 10/15\n",
            "258311/258311 [==============================] - 198s 766us/step - loss: 0.1408 - binary_accuracy: 0.9407 - val_loss: 0.7797 - val_binary_accuracy: 0.7884\n",
            "Epoch 11/15\n",
            "258311/258311 [==============================] - 198s 766us/step - loss: 0.1283 - binary_accuracy: 0.9463 - val_loss: 0.7797 - val_binary_accuracy: 0.7967\n",
            "Epoch 12/15\n",
            "258311/258311 [==============================] - 198s 768us/step - loss: 0.1182 - binary_accuracy: 0.9510 - val_loss: 0.8324 - val_binary_accuracy: 0.7970\n",
            "Epoch 13/15\n",
            "258311/258311 [==============================] - 198s 768us/step - loss: 0.1102 - binary_accuracy: 0.9546 - val_loss: 0.8547 - val_binary_accuracy: 0.7945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f060800fd30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "qi7qLeB45OOA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = load_model('embedding_NN_model_v7.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dll6fji7E9nx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1a2a0d25-3dc2-45a0-ab3c-b3f9c49ca884"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(x=list(val[:-1]),y=val[IS_DUPLICATE], batch_size=BATCH_SIZE)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64578/64578 [==============================] - 16s 242us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4599608550606606, 0.7913221220941504]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "UzTgNh98mWgv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predicted = model.predict(list(test), batch_size=BATCH_SIZE)\n",
        "predicted = predicted.ravel()\n",
        "predicted = list(map(lambda x: 1 if x > 0.5 else 0, predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m_S3G8MfFyd_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import datetime\n",
        "from os.path import join,abspath,curdir\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "'''\n",
        "Use this with \n",
        "import sys\n",
        "sys.path.insert(0, './common/')\n",
        "import csv_utils\n",
        "csv_utils.create_csvs(predicted, test_ids)\n",
        "\n",
        "Given the predicted outputs for each model:\n",
        "predicted = [[0,1,0,0,1,0],[0,1,0,1,1,0],[0,1,0,0,1,1]]\n",
        "test_ids = [12,32,43,44,11]\n",
        "Create the csvs to submit to kaggle\n",
        "'''\n",
        "\n",
        "def create_csvs(predicted, test_ids):\n",
        "    EXPECTED_ROWS = 81126 \n",
        "    tests_ids_len = len(test_ids)\n",
        "    assert(tests_ids_len == EXPECTED_ROWS)\n",
        "    assert(len(predicted)==tests_ids_len)\n",
        "    \n",
        "    CURRENT_PATH = abspath(curdir)\n",
        "    \n",
        "    merged = {'test_id': test_ids}\n",
        "    merged['is_duplicate'] = predicted\n",
        "\n",
        "    FILENAME = 'submission_' + datetime.datetime.now().strftime(\"%I%M%p-%B-%d-%Y\") + '.csv'\n",
        "    df = pd.DataFrame.from_dict(merged)\n",
        "\n",
        "    df.set_index('test_id', inplace=True)\n",
        "\n",
        "    FULL_PATH = join(CURRENT_PATH, FILENAME)\n",
        "\n",
        "    df.to_csv(path_or_buf=FULL_PATH, sep=',')\n",
        "\n",
        "    print('saved in: ', FULL_PATH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i3GwGHbWGWt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da427027-5b7a-42c9-a528-a1ca38cdd1fe"
      },
      "cell_type": "code",
      "source": [
        "create_csvs(predicted, test_data.test_id.values)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved in:  /content/submission_1114PM-December-08-2018.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BKLaoX0zGZMu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}