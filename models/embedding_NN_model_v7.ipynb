{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embedding_NN_model_v7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "aXioKJ6V7kFP",
        "colab_type": "code",
        "outputId": "38baf19e-f19e-4dda-cce8-53323efb1a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D, BatchNormalization\n",
        "from keras.layers import Concatenate, Subtract, Add, Multiply\n",
        "from keras.layers import Input, Dropout, PReLU, SpatialDropout1D\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import keras.backend as K\n",
        "from keras import optimizers\n",
        "\n",
        "import os\n",
        "import io"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "l6HoV37h7m-C",
        "colab_type": "code",
        "outputId": "d934d564-e123-4932-b775-14d16bf6b513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Loading drive content\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t1-hbHHz7rHi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = os.path.join('drive', 'My Drive', 'quora', 'train_data_v3_processed (1).csv')\n",
        "train_data = pd.read_csv(path, dtype={'question1': 'str', 'question2': 'str'})\n",
        "\n",
        "path = os.path.join('drive', 'My Drive', 'quora', 'test_data_v3_processed.csv')\n",
        "test_data = pd.read_csv(path, dtype={'question1': 'str', 'question2': 'str'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sse46ljR9fHn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Temporal\n",
        "test_data.drop(['num_stop_words_q1', 'num_stop_words_q2', 'numerics_q1', 'numerics_q2', 'uppercase_q1', 'uppercase_q2'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nAopLIhE_r6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "63eb79a3-2f55-42be-9da3-a8610ccf9195"
      },
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "      <th>question1_lemma</th>\n",
              "      <th>question1_tag</th>\n",
              "      <th>question2_lemma</th>\n",
              "      <th>question2_tag</th>\n",
              "      <th>fuzz_ratio</th>\n",
              "      <th>fuzz_partial_ratio</th>\n",
              "      <th>...</th>\n",
              "      <th>len_q1</th>\n",
              "      <th>len_q2</th>\n",
              "      <th>diff_len</th>\n",
              "      <th>len_char_q1</th>\n",
              "      <th>len_char_q2</th>\n",
              "      <th>len_word_q1</th>\n",
              "      <th>len_word_q2</th>\n",
              "      <th>common_words</th>\n",
              "      <th>avg_word_q1</th>\n",
              "      <th>avg_word_q2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "      <td>what be the step by step guide to invest in sh...</td>\n",
              "      <td>WP VBZ DT NN IN NN NN TO VB IN NN NN IN NN .</td>\n",
              "      <td>what be the step by step guide to invest in sh...</td>\n",
              "      <td>WP VBZ DT NN IN NN NN TO VB IN NN NN .</td>\n",
              "      <td>93</td>\n",
              "      <td>98</td>\n",
              "      <td>...</td>\n",
              "      <td>66</td>\n",
              "      <td>57</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>14</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>3.785714</td>\n",
              "      <td>3.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "      <td>what be the story of kohinoor ( koh - i - noor...</td>\n",
              "      <td>WP VBZ DT NN IN NNP -LRB- NNP HYPH NNP HYPH NN...</td>\n",
              "      <td>what would happen if the indian government ste...</td>\n",
              "      <td>WP MD VB IN DT JJ NN VBD DT NNP -LRB- NNP HYPH...</td>\n",
              "      <td>68</td>\n",
              "      <td>78</td>\n",
              "      <td>...</td>\n",
              "      <td>51</td>\n",
              "      <td>88</td>\n",
              "      <td>-37</td>\n",
              "      <td>21</td>\n",
              "      <td>29</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>11</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>5.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "      <td>how can -PRON- increase the speed of -PRON- in...</td>\n",
              "      <td>WRB MD PRP VB DT NN IN PRP$ NN NN IN VBG DT NNP .</td>\n",
              "      <td>how can internet speed be increase by hack thr...</td>\n",
              "      <td>WRB MD VB NN VB VBN IN VBG IN NN .</td>\n",
              "      <td>35</td>\n",
              "      <td>44</td>\n",
              "      <td>...</td>\n",
              "      <td>73</td>\n",
              "      <td>59</td>\n",
              "      <td>14</td>\n",
              "      <td>25</td>\n",
              "      <td>24</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>4.285714</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "      <td>why be -PRON- mentally very lonely ? how can -...</td>\n",
              "      <td>WRB VBP PRP RB RB JJ . WRB MD PRP VB PRP .</td>\n",
              "      <td>find the remainder when [ math]23^{24}[/math ]...</td>\n",
              "      <td>VB DT NN WRB -LRB- NN -RRB- VBZ VBN IN CD .</td>\n",
              "      <td>17</td>\n",
              "      <td>22</td>\n",
              "      <td>...</td>\n",
              "      <td>50</td>\n",
              "      <td>65</td>\n",
              "      <td>-15</td>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>3.636364</td>\n",
              "      <td>6.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "      <td>which one dissolve in water quikly sugar , sal...</td>\n",
              "      <td>WDT CD NN IN NN RB NN , NN , NN CC NN FW NN .</td>\n",
              "      <td>which fish would survive in salt water ?</td>\n",
              "      <td>WDT NN MD VB IN NN NN .</td>\n",
              "      <td>37</td>\n",
              "      <td>55</td>\n",
              "      <td>...</td>\n",
              "      <td>76</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>25</td>\n",
              "      <td>18</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4.923077</td>\n",
              "      <td>4.714286</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                          question1  \\\n",
              "0   0  What is the step by step guide to invest in sh...   \n",
              "1   1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
              "2   2  How can I increase the speed of my internet co...   \n",
              "3   3  Why am I mentally very lonely? How can I solve...   \n",
              "4   4  Which one dissolve in water quikly sugar, salt...   \n",
              "\n",
              "                                           question2  is_duplicate  \\\n",
              "0  What is the step by step guide to invest in sh...             0   \n",
              "1  What would happen if the Indian government sto...             0   \n",
              "2  How can Internet speed be increased by hacking...             0   \n",
              "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
              "4            Which fish would survive in salt water?             0   \n",
              "\n",
              "                                     question1_lemma  \\\n",
              "0  what be the step by step guide to invest in sh...   \n",
              "1  what be the story of kohinoor ( koh - i - noor...   \n",
              "2  how can -PRON- increase the speed of -PRON- in...   \n",
              "3  why be -PRON- mentally very lonely ? how can -...   \n",
              "4  which one dissolve in water quikly sugar , sal...   \n",
              "\n",
              "                                       question1_tag  \\\n",
              "0       WP VBZ DT NN IN NN NN TO VB IN NN NN IN NN .   \n",
              "1  WP VBZ DT NN IN NNP -LRB- NNP HYPH NNP HYPH NN...   \n",
              "2  WRB MD PRP VB DT NN IN PRP$ NN NN IN VBG DT NNP .   \n",
              "3         WRB VBP PRP RB RB JJ . WRB MD PRP VB PRP .   \n",
              "4      WDT CD NN IN NN RB NN , NN , NN CC NN FW NN .   \n",
              "\n",
              "                                     question2_lemma  \\\n",
              "0  what be the step by step guide to invest in sh...   \n",
              "1  what would happen if the indian government ste...   \n",
              "2  how can internet speed be increase by hack thr...   \n",
              "3  find the remainder when [ math]23^{24}[/math ]...   \n",
              "4           which fish would survive in salt water ?   \n",
              "\n",
              "                                       question2_tag  fuzz_ratio  \\\n",
              "0             WP VBZ DT NN IN NN NN TO VB IN NN NN .          93   \n",
              "1  WP MD VB IN DT JJ NN VBD DT NNP -LRB- NNP HYPH...          68   \n",
              "2                 WRB MD VB NN VB VBN IN VBG IN NN .          35   \n",
              "3        VB DT NN WRB -LRB- NN -RRB- VBZ VBN IN CD .          17   \n",
              "4                            WDT NN MD VB IN NN NN .          37   \n",
              "\n",
              "   fuzz_partial_ratio     ...       len_q1  len_q2  diff_len  len_char_q1  \\\n",
              "0                  98     ...           66      57         9           20   \n",
              "1                  78     ...           51      88       -37           21   \n",
              "2                  44     ...           73      59        14           25   \n",
              "3                  22     ...           50      65       -15           19   \n",
              "4                  55     ...           76      39        37           25   \n",
              "\n",
              "   len_char_q2  len_word_q1  len_word_q2  common_words  avg_word_q1  \\\n",
              "0           20           14           12            12     3.785714   \n",
              "1           29            8           13            11     5.500000   \n",
              "2           24           14           10             6     4.285714   \n",
              "3           26           11            9             2     3.636364   \n",
              "4           18           13            7             5     4.923077   \n",
              "\n",
              "   avg_word_q2  \n",
              "0     3.833333  \n",
              "1     5.846154  \n",
              "2     5.000000  \n",
              "3     6.333333  \n",
              "4     4.714286  \n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "UhzkqsbS73hL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip glove.840B.300d.zip -d ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uJxPiNuEEcSr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RAW_Q1 = 0\n",
        "RAW_Q2 = 1\n",
        "LEMMA_Q1 = 2\n",
        "LEMMA_Q2 = 3\n",
        "TAGS_Q1 = 4\n",
        "TAGS_Q2 = 5\n",
        "FEATURES = 6\n",
        "IS_DUPLICATE = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uUN1R6IgEw7Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_questions(q1_train, q2_train, q1_val, q2_val, q1_test, q2_test):\n",
        "    tokenizer = Tokenizer(lower=False)\n",
        "    # Training the tokenizer with the words from all questions from training\n",
        "    tokenizer.fit_on_texts(np.concatenate((q1_train, q2_train), axis=0))\n",
        "\n",
        "    # Convert each word to a integer according to the tokenizer\n",
        "    q1_train = tokenizer.texts_to_sequences(q1_train)\n",
        "    q2_train = tokenizer.texts_to_sequences(q2_train)\n",
        "    q1_val = tokenizer.texts_to_sequences(q1_val)\n",
        "    q2_val = tokenizer.texts_to_sequences(q2_val)\n",
        "    q1_test = tokenizer.texts_to_sequences(q1_test)\n",
        "    q2_test = tokenizer.texts_to_sequences(q2_test)\n",
        "\n",
        "    # Add a left pad to make all the question have the same length\n",
        "    q1_train = pad_sequences(q1_train, maxlen=MAX_LENGTH)\n",
        "    q2_train = pad_sequences(q2_train, maxlen=MAX_LENGTH)\n",
        "    q1_val = pad_sequences(q1_val, maxlen=MAX_LENGTH)\n",
        "    q2_val = pad_sequences(q2_val, maxlen=MAX_LENGTH)\n",
        "    q1_test = pad_sequences(q1_test, maxlen=MAX_LENGTH)\n",
        "    q2_test = pad_sequences(q2_test, maxlen=MAX_LENGTH)\n",
        "    \n",
        "    return q1_train, q2_train, q1_val, q2_val, q1_test, q2_test, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q1oAiHb5BJXu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 35\n",
        "'''\n",
        "input:\n",
        "    - train: raw text of training\n",
        "    - test: raw text of testing\n",
        "ouput:\n",
        "    - train: processed training\n",
        "    - test: processed testing\n",
        "    - vocab: Number of vocaboluary\n",
        "'''\n",
        "def prep_data(train, val, test):\n",
        "    \n",
        "    raw_q1_train, raw_q2_train,\\\n",
        "    raw_q1_val, raw_q2_val,\\\n",
        "    raw_q1_test, raw_q2_test,\\\n",
        "    raw_tokenizer = process_questions(train[RAW_Q1],\n",
        "                                      train[RAW_Q2],\n",
        "                                      val[RAW_Q1],\n",
        "                                      val[RAW_Q2],\n",
        "                                      test[RAW_Q1],\n",
        "                                      test[RAW_Q2])\n",
        "    \n",
        "    lemma_q1_train, lemma_q2_train,\\\n",
        "    lemma_q1_val, lemma_q2_val,\\\n",
        "    lemma_q1_test, lemma_q2_test,\\\n",
        "    lemma_tokenizer = process_questions(train[LEMMA_Q1],\n",
        "                                        train[LEMMA_Q2],\n",
        "                                        val[LEMMA_Q1],\n",
        "                                        val[LEMMA_Q2],\n",
        "                                        test[LEMMA_Q1],\n",
        "                                        test[LEMMA_Q2])\n",
        "    \n",
        "    tags_q1_train, tags_q2_train,\\\n",
        "    tags_q1_val, tags_q2_val,\\\n",
        "    tags_q1_test, tags_q2_test,\\\n",
        "    tags_tokenizer = process_questions(train[TAGS_Q1],\n",
        "                                        train[TAGS_Q2],\n",
        "                                        val[TAGS_Q1],\n",
        "                                        val[TAGS_Q2],\n",
        "                                        test[TAGS_Q1],\n",
        "                                        test[TAGS_Q2])\n",
        "\n",
        "    train = raw_q1_train, raw_q2_train, lemma_q1_train, lemma_q2_train, tags_q1_train, tags_q2_train, np.array(train[FEATURES].tolist()), train[IS_DUPLICATE]\n",
        "    val = raw_q1_val, raw_q2_val, lemma_q1_val, lemma_q2_val, tags_q1_val, tags_q2_val, np.array(val[FEATURES].tolist()), val[IS_DUPLICATE]\n",
        "    test = raw_q1_test, raw_q2_test, lemma_q1_test, lemma_q2_test, tags_q1_test, tags_q2_test, np.array(test[FEATURES].tolist())\n",
        "    \n",
        "    return train, val, test, raw_tokenizer, len(lemma_tokenizer.word_index) + 1, len(tags_tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BaZlHfayCgRX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prep_embd(fname, tokenizer):\n",
        "    f = open(fname,'r')\n",
        "    d = 300\n",
        "    \n",
        "    vocab = len(tokenizer.word_index) + 1\n",
        "    embedding_matrix = np.zeros((vocab, d))\n",
        "    \n",
        "    for line in f:\n",
        "        tokens = line.split(' ')\n",
        "        word = tokens[0]\n",
        "        if word in tokenizer.word_index:\n",
        "            i = tokenizer.word_index[word]\n",
        "            vector = np.asarray(tokens[1:], dtype='float32')\n",
        "            embedding_matrix[i] = vector\n",
        "            \n",
        "    return vocab, d, embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "brhoD9i3P2iR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_features(df):\n",
        "    df = df.drop(['question1', 'question2', \n",
        "                  'question1_lemma', 'question2_lemma', \n",
        "                  'question1_tag', 'question2_tag'], axis=1)\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(['id', 'is_duplicate'], axis=1)\n",
        "    elif 'test_id' in df.columns:\n",
        "        df = df.drop('test_id', axis=1)\n",
        "    return df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C7oE9X_sCiPN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_features = get_features(train_data)\n",
        "train_data['features'] = train_features.tolist()\n",
        "data = train_data[['question1', 'question2', \n",
        "                   'question1_lemma', 'question2_lemma', \n",
        "                   'question1_tag', 'question2_tag', \n",
        "                   'features', 'is_duplicate']].values\n",
        "\n",
        "train, val = train_test_split(data, test_size=0.2, random_state=19)\n",
        "train = train.T\n",
        "val = val.T\n",
        "\n",
        "test_features = get_features(test_data)\n",
        "test_data['features'] = test_features.tolist()\n",
        "test = test_data[['question1', 'question2', \n",
        "                  'question1_lemma', 'question2_lemma', \n",
        "                  'question1_tag', 'question2_tag', \n",
        "                  'features']].values\n",
        "test = test.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvCXA58kHSPQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train, val, test, raw_tokenizer, LEMMA_VOCAB, TAGS_VOCAB = prep_data(train, val, test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q93H7PCDM-1Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = os.path.join('glove.840B.300d.txt')\n",
        "RAW_VOCAB, GLOVE_EMBEDDING_DIM, GLOVE_EMBEDDING_MATRIX = prep_embd(path, raw_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f735o3YlM-6x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 200\n",
        "DROPOUT_RATE = 0.2\n",
        "FILTERS = 16\n",
        "KERNEL_SIZE = 5\n",
        "\n",
        "def get_model_non_trainable_embeddings(input_q):\n",
        "    embd = Embedding(input_dim = RAW_VOCAB,\n",
        "                     output_dim = GLOVE_EMBEDDING_DIM, \n",
        "                     weights = [GLOVE_EMBEDDING_MATRIX],\n",
        "                     trainable = False,\n",
        "                     input_length=MAX_LENGTH)(input_q)\n",
        "    lstm = LSTM(EMBEDDING_DIM, \n",
        "                recurrent_dropout = DROPOUT_RATE)(embd)\n",
        "    flatten = Flatten()(embd)\n",
        "    return flatten\n",
        "\n",
        "def get_model_embeddings(input_q, vocab):\n",
        "    embd = Embedding(input_dim = vocab,\n",
        "                     output_dim = EMBEDDING_DIM, \n",
        "                     input_length=MAX_LENGTH)(input_q)\n",
        "    \n",
        "    dropout = Dropout(DROPOUT_RATE)(embd)\n",
        "    flatten = Flatten()(dropout)\n",
        "    return flatten\n",
        "  \n",
        "def get_model_features(features_input):\n",
        "    model = BatchNormalization()(features_input)\n",
        "    for i in range(4):\n",
        "        model = Dense(units = 200, activation='relu')(model)\n",
        "        model = Dropout(DROPOUT_RATE)(model)\n",
        "    return model\n",
        "\n",
        "def get_model():\n",
        "    # Define inputs\n",
        "    raw_input_q1 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    raw_input_q2 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    \n",
        "    lemma_input_q1 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    lemma_input_q2 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    \n",
        "    tags_input_q1 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    tags_input_q2 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
        "    \n",
        "    features_input = Input(shape=(train_features.shape[1],), dtype='float32')\n",
        "   \n",
        "    # Load models\n",
        "    model_raw_q1 = get_model_non_trainable_embeddings(raw_input_q1)\n",
        "    model_raw_q2 = get_model_non_trainable_embeddings(raw_input_q2)\n",
        "    \n",
        "    model_lemma_q1 = get_model_embeddings(lemma_input_q1, LEMMA_VOCAB)\n",
        "    model_lemma_q2 = get_model_embeddings(lemma_input_q2, LEMMA_VOCAB)\n",
        "    \n",
        "    model_tags_q1 = get_model_embeddings(tags_input_q1, TAGS_VOCAB)\n",
        "    model_tags_q2 = get_model_embeddings(tags_input_q2, TAGS_VOCAB) \n",
        "    \n",
        "    model_features = get_model_features(features_input)\n",
        "    \n",
        "    # Merge models\n",
        "    raw_subtract = Subtract()([model_raw_q2, model_raw_q1])\n",
        "    lemma_subtract = Subtract()([model_lemma_q2, model_lemma_q1])\n",
        "    tags_subtract = Subtract()([model_tags_q2, model_tags_q1])\n",
        "    \n",
        "    concat = Concatenate()([raw_subtract, lemma_subtract, tags_subtract, model_features])\n",
        "    concat = BatchNormalization()(concat)\n",
        "    concat = Dropout(DROPOUT_RATE)(concat)\n",
        "    \n",
        "    for i in range(3):\n",
        "        concat = Dense(units = 200, activation='relu')(concat)\n",
        "        concat = Dropout(DROPOUT_RATE)(concat)\n",
        "\n",
        "    output = Dense(1, activation='sigmoid')(concat)\n",
        "    \n",
        "    model = Model(inputs=[raw_input_q1, raw_input_q2, \n",
        "                          lemma_input_q1, lemma_input_q2, \n",
        "                          tags_input_q1, tags_input_q2,\n",
        "                          features_input], \n",
        "                  outputs=output)\n",
        "    \n",
        "    model.compile(optimizer=optimizers.Nadam(lr=0.0005),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['binary_accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AZIEWAycM-_o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = get_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8MVK-m9j9tE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "d47727c2-a95a-4ae1-bdd0-075ed2d828d7"
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "model_checkpoint = ModelCheckpoint('embedding_NN_model_v7.h5', save_best_only=True, save_weights_only=False, monitor='val_binary_accuracy', mode='max')\n",
        "\n",
        "model.fit(list(train[:-1]), train[IS_DUPLICATE],\n",
        "          validation_data = (list(val[:-1]),val[IS_DUPLICATE]),\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=15,\n",
        "          callbacks=[early_stopping, model_checkpoint])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 258311 samples, validate on 64578 samples\n",
            "Epoch 1/15\n",
            "258311/258311 [==============================] - 64s 248us/step - loss: 0.4612 - binary_accuracy: 0.7653 - val_loss: 0.4027 - val_binary_accuracy: 0.8045\n",
            "Epoch 2/15\n",
            "258311/258311 [==============================] - 54s 207us/step - loss: 0.3472 - binary_accuracy: 0.8372 - val_loss: 0.3784 - val_binary_accuracy: 0.8189\n",
            "Epoch 3/15\n",
            "258311/258311 [==============================] - 54s 211us/step - loss: 0.2733 - binary_accuracy: 0.8760 - val_loss: 0.3915 - val_binary_accuracy: 0.8146\n",
            "Epoch 4/15\n",
            "258311/258311 [==============================] - 54s 209us/step - loss: 0.2198 - binary_accuracy: 0.9025 - val_loss: 0.4213 - val_binary_accuracy: 0.8188\n",
            "Epoch 5/15\n",
            "258311/258311 [==============================] - 54s 209us/step - loss: 0.1825 - binary_accuracy: 0.9210 - val_loss: 0.4561 - val_binary_accuracy: 0.8199\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ffa16a5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "xssB758W28Qf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = load_model('embedding_NN_model_v7.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "esRzzXjA2-L2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b17b500d-5bf5-49cd-b5f0-050aa7aeb05c"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(x=list(val[:-1]),y=val[IS_DUPLICATE], batch_size=BATCH_SIZE)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64578/64578 [==============================] - 3s 39us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.456110086419516, 0.8198767381877528]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "UzTgNh98mWgv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predicted = model.predict(list(test), batch_size=BATCH_SIZE)\n",
        "predicted = predicted.ravel()\n",
        "predicted = list(map(lambda x: 1 if x > 0.5 else 0, predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F7ajzqj_3Blm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import datetime\n",
        "from os.path import join,abspath,curdir\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "'''\n",
        "Use this with \n",
        "import sys\n",
        "sys.path.insert(0, './common/')\n",
        "import csv_utils\n",
        "csv_utils.create_csvs(predicted, test_ids)\n",
        "\n",
        "Given the predicted outputs for each model:\n",
        "predicted = [[0,1,0,0,1,0],[0,1,0,1,1,0],[0,1,0,0,1,1]]\n",
        "test_ids = [12,32,43,44,11]\n",
        "Create the csvs to submit to kaggle\n",
        "'''\n",
        "\n",
        "def create_csvs(predicted, test_ids):\n",
        "    EXPECTED_ROWS = 81126 \n",
        "    tests_ids_len = len(test_ids)\n",
        "    assert(tests_ids_len == EXPECTED_ROWS)\n",
        "    assert(len(predicted)==tests_ids_len)\n",
        "    \n",
        "    CURRENT_PATH = abspath(curdir)\n",
        "    \n",
        "    merged = {'test_id': test_ids}\n",
        "    merged['is_duplicate'] = predicted\n",
        "\n",
        "    FILENAME = 'submission_' + datetime.datetime.now().strftime(\"%I%M%p-%B-%d-%Y\") + '.csv'\n",
        "    df = pd.DataFrame.from_dict(merged)\n",
        "\n",
        "    df.set_index('test_id', inplace=True)\n",
        "\n",
        "    FULL_PATH = join(CURRENT_PATH, FILENAME)\n",
        "\n",
        "    df.to_csv(path_or_buf=FULL_PATH, sep=',')\n",
        "\n",
        "    print('saved in: ', FULL_PATH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7baMSlJU3CXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7bcb5a46-6f92-46b7-daa5-b2e0fa860c5e"
      },
      "cell_type": "code",
      "source": [
        "create_csvs(predicted, test_data.test_id.values)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved in:  /content/submission_0135PM-December-09-2018.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fGWXYzG5LbFo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}